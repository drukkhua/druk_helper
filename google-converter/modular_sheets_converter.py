#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å–Ω—ã–π Google Sheets Converter —Å Batch API
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∞ –Ω–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è –ª–µ–≥–∫–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
"""

import hashlib
import json
import os
import re
import sys
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Any, Dict, List, Optional, Protocol, Tuple

import pandas as pd
import requests
import yaml


# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
try:
    import openpyxl


    XLSX_AVAILABLE = True
except ImportError:
    XLSX_AVAILABLE = False

try:
    from slugify import slugify


    SLUGIFY_AVAILABLE = True
except ImportError:
    SLUGIFY_AVAILABLE = False


# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø / CONFIGURATION
# ============================================================================

class Config:
    """–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"""

    def __init__(self,
                 api_key: Optional[str] = None,
                 output_dir: str = "output",
                 cache_file: str = "cache/sheets_cache.json",
                 batch_size: int = 100,
                 timeout: int = 60):
        # API –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.api_key = api_key or os.getenv('GOOGLE_SHEETS_API_KEY')

        # –ü—É—Ç–∏ –∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.output_dir = output_dir
        self.cache_file = cache_file

        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        self.batch_size = batch_size  # –ú–∞–∫—Å–∏–º—É–º –ª–∏—Å—Ç–æ–≤ –≤ –æ–¥–Ω–æ–º batch –∑–∞–ø—Ä–æ—Å–µ
        self.timeout = timeout

        # API URLs
        self.sheets_api_base = "https://sheets.googleapis.com/v4/spreadsheets"
        self.export_api_base = "https://docs.google.com/spreadsheets/d"

        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not self.api_key:
            print("‚ö†Ô∏è Google Sheets API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—É–±–ª–∏—á–Ω—ã–π —Ä–µ–∂–∏–º.")


# ============================================================================
# –ü–†–û–¢–û–ö–û–õ–´ –ò –ò–ù–¢–ï–†–§–ï–ô–°–´ / PROTOCOLS & INTERFACES  
# ============================================================================

class DataExporter(Protocol):
    """–ü—Ä–æ—Ç–æ–∫–æ–ª –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""

    def export(self, data: pd.DataFrame, filepath: str, **kwargs) -> None:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª"""
        ...


class CacheProvider(Protocol):
    """–ü—Ä–æ—Ç–æ–∫–æ–ª –¥–ª—è –∫–µ—à-–ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""

    def get(self, key: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫–µ—à–∞"""
        ...

    def set(self, key: str, value: Any) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫–µ—à"""
        ...

    def is_changed(self, key: str, data: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ"""
        ...


class DataProcessor(Protocol):
    """–ü—Ä–æ—Ç–æ–∫–æ–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""

    def process(self, data: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ"""
        ...


# ============================================================================
# GOOGLE SHEETS API CLIENT / –ö–õ–ò–ï–ù–¢ API
# ============================================================================

class GoogleSheetsClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Google Sheets API —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π batch –∑–∞–ø—Ä–æ—Å–æ–≤"""

    def __init__(self, config: Config):
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ModularSheetsConverter/2.0'
        })

    def extract_sheet_id(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç ID —Ç–∞–±–ª–∏—Ü—ã –∏–∑ URL"""
        pattern = r'/spreadsheets/d/([a-zA-Z0-9-_]+)'
        match = re.search(pattern, url)
        if not match:
            raise ValueError(f"Invalid Google Sheets URL: {url}")
        return match.group(1)

    def get_sheets_metadata(self, sheet_id: str) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö –ª–∏—Å—Ç–æ–≤ —Ç–∞–±–ª–∏—Ü—ã
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: [{'gid': str, 'name': str, 'rows': int, 'cols': int}, ...]
        """
        print("üîç –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ª–∏—Å—Ç–æ–≤...")

        api_url = f"{self.config.sheets_api_base}/{sheet_id}"
        params = {'fields': 'sheets.properties'}

        if self.config.api_key:
            params['key'] = self.config.api_key

        try:
            response = self.session.get(api_url, params=params, timeout=self.config.timeout)
            response.raise_for_status()

            data = response.json()
            sheets_info = []

            for sheet in data.get('sheets', []):
                props = sheet.get('properties', {})
                grid_props = props.get('gridProperties', {})

                sheets_info.append({
                    'gid': str(props.get('sheetId', 0)),
                    'name': props.get('title', f'Sheet{props.get("sheetId", 0)}'),
                    'rows': grid_props.get('rowCount', 1000),
                    'cols': grid_props.get('columnCount', 26)
                })

            print(f"üìã –ù–∞–π–¥–µ–Ω–æ –ª–∏—Å—Ç–æ–≤: {len(sheets_info)}")
            for i, sheet in enumerate(sheets_info, 1):
                print(f"   {i}. '{sheet['name']}' ({sheet['rows']}x{sheet['cols']})")

            return sheets_info

        except requests.RequestException as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            return [{'gid': '0', 'name': 'Sheet1', 'rows': 1000, 'cols': 26}]

    def batch_get_sheets_data(self, sheet_id: str, sheets_info: List[Dict]) -> Dict[str, Tuple[pd.DataFrame, str]]:
        """
        üöÄ BATCH API: –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –í–°–ï–• –ª–∏—Å—Ç–æ–≤ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ 100+ –ª–∏—Å—Ç–æ–≤ –≤ –æ–¥–Ω–æ–º batch –∑–∞–ø—Ä–æ—Å–µ
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: Dict[sheet_name, (DataFrame, raw_data)]
        """
        results = {}

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏ –µ—Å–ª–∏ –ª–∏—Å—Ç–æ–≤ –±–æ–ª—å—à–µ —á–µ–º batch_size
        batches = [sheets_info[i:i + self.config.batch_size]
                   for i in range(0, len(sheets_info), self.config.batch_size)]

        print(f"üöÄ Batch –∑–∞–≥—Ä—É–∑–∫–∞: {len(sheets_info)} –ª–∏—Å—Ç–æ–≤ –≤ {len(batches)} –∑–∞–ø—Ä–æ—Å–∞—Ö")

        for batch_num, batch_sheets in enumerate(batches, 1):
            print(f"üì¶ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º batch {batch_num}/{len(batches)} ({len(batch_sheets)} –ª–∏—Å—Ç–æ–≤)")

            batch_results = self._load_single_batch(sheet_id, batch_sheets)
            results.update(batch_results)

        print(f"‚úÖ Batch –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(results)} –ª–∏—Å—Ç–æ–≤ –ø–æ–ª—É—á–µ–Ω–æ")
        return results

    def _load_single_batch(self, sheet_id: str, sheets_info: List[Dict]) -> Dict[str, Tuple[pd.DataFrame, str]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–¥–∏–Ω batch –ª–∏—Å—Ç–æ–≤"""
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º ranges –¥–ª—è batch –∑–∞–ø—Ä–æ—Å–∞
            ranges = []
            sheet_mapping = {}

            for i, sheet_info in enumerate(sheets_info):
                sheet_name = sheet_info['name']
                # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –∏–º–µ–Ω–∞ –ª–∏—Å—Ç–æ–≤ —Å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏
                escaped_name = self._escape_sheet_name(sheet_name)

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –ª–∏—Å—Ç–∞
                rows = min(sheet_info.get('rows', 1000), 10000)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º
                cols = min(sheet_info.get('cols', 26), 26)  # –î–æ –∫–æ–ª–æ–Ω–∫–∏ Z
                col_letter = chr(ord('A') + cols - 1)

                range_name = f"{escaped_name}!A1:{col_letter}{rows}"
                ranges.append(range_name)
                sheet_mapping[i] = sheet_name

            # –í—ã–ø–æ–ª–Ω—è–µ–º batch –∑–∞–ø—Ä–æ—Å
            api_url = f"{self.config.sheets_api_base}/{sheet_id}/values:batchGet"
            params = {
                'ranges': ranges,
                'valueRenderOption': 'UNFORMATTED_VALUE',
                'dateTimeRenderOption': 'FORMATTED_STRING'
            }

            if self.config.api_key:
                params['key'] = self.config.api_key

            response = self.session.get(api_url, params=params, timeout=self.config.timeout)

            if response.status_code == 429:
                print("‚è≥ Rate limit –¥–æ—Å—Ç–∏–≥–Ω—É—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º CSV fallback...")
                return self._csv_fallback_batch(sheet_id, sheets_info)

            response.raise_for_status()
            batch_data = response.json()

            return self._process_batch_response(batch_data, sheet_mapping)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ batch –∑–∞–ø—Ä–æ—Å–∞: {e}")
            print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CSV fallback...")
            return self._csv_fallback_batch(sheet_id, sheets_info)

    def _process_batch_response(self, batch_data: Dict, sheet_mapping: Dict[int, str]) -> Dict[
        str, Tuple[pd.DataFrame, str]]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç batch API"""
        results = {}
        value_ranges = batch_data.get('valueRanges', [])

        for i, value_range in enumerate(value_ranges):
            sheet_name = sheet_mapping.get(i, f"Sheet{i}")

            try:
                values = value_range.get('values', [])
                raw_data = json.dumps(values, ensure_ascii=False)

                if not values:
                    results[sheet_name] = (pd.DataFrame(), raw_data)
                    continue

                # –°–æ–∑–¥–∞–µ–º DataFrame
                if len(values) == 1:
                    # –¢–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                    df = pd.DataFrame(columns=values[0])
                else:
                    # –ï—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
                    headers = values[0]
                    data_rows = values[1:]

                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª–∏–Ω—É —Å—Ç—Ä–æ–∫
                    max_cols = len(headers)
                    normalized_rows = []
                    for row in data_rows:
                        while len(row) < max_cols:
                            row.append('')
                        normalized_rows.append(row[:max_cols])

                    df = pd.DataFrame(normalized_rows, columns=headers)

                # –û—á–∏—â–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                df = df.dropna(how='all')

                print(f"‚úÖ [{sheet_name}] {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
                results[sheet_name] = (df, raw_data)

            except Exception as e:
                print(f"‚ùå [{sheet_name}] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                results[sheet_name] = (pd.DataFrame(), "")

        return results

    def _csv_fallback_batch(self, sheet_id: str, sheets_info: List[Dict]) -> Dict[str, Tuple[pd.DataFrame, str]]:
        """Fallback: –∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ CSV export"""
        results = {}
        print(f"üì• CSV fallback –¥–ª—è {len(sheets_info)} –ª–∏—Å—Ç–æ–≤...")

        for sheet_info in sheets_info:
            gid = sheet_info['gid']
            sheet_name = sheet_info['name']

            try:
                csv_url = f"{self.config.export_api_base}/{sheet_id}/export"
                params = {'format': 'csv', 'gid': gid}

                response = self.session.get(csv_url, params=params, timeout=30)
                response.raise_for_status()

                # –ß–∏—Ç–∞–µ–º CSV
                from io import StringIO
                csv_data = response.content.decode('utf-8')
                df = pd.read_csv(StringIO(csv_data))
                df = df.dropna(how='all')

                print(f"‚úÖ [{sheet_name}] {len(df)} —Å—Ç—Ä–æ–∫ (CSV)")
                results[sheet_name] = (df, csv_data)

            except Exception as e:
                print(f"‚ùå [{sheet_name}] CSV –æ—à–∏–±–∫–∞: {e}")
                results[sheet_name] = (pd.DataFrame(), "")

        return results

    def _escape_sheet_name(self, sheet_name: str) -> str:
        """–≠–∫—Ä–∞–Ω–∏—Ä—É–µ—Ç –∏–º—è –ª–∏—Å—Ç–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ API"""
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –∑–∞–∫–ª—é—á–∞–µ–º –≤ –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏
        if any(c in sheet_name for c in " '\"!@#$%^&*()"):
            return f"'{sheet_name.replace(\"'\", \"''\")}'"
        return sheet_name


# ============================================================================
# –°–ò–°–¢–ï–ú–ê –ö–ï–®–ò–†–û–í–ê–ù–ò–Ø / CACHING SYSTEM
# ============================================================================

class FileCache:
    """–§–∞–π–ª–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""

    def __init__(self, cache_file: str):
        self.cache_file = cache_file
        self.cache_data = {}
        self._load_cache()

    def _load_cache(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–µ—à –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.cache_data = json.load(f)
                print(f"üìÇ –ö–µ—à –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.cache_data)} –∑–∞–ø–∏—Å–µ–π")
            else:
                self.cache_data = {}
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–µ—à–∞: {e}")
            self.cache_data = {}

    def _save_cache(self) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–µ—à –≤ —Ñ–∞–π–ª"""
        try:
            os.makedirs(os.path.dirname(self.cache_file), exist_ok=True)
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–µ—à–∞: {e}")

    def get(self, key: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫–µ—à–∞"""
        return self.cache_data.get(key)

    def set(self, key: str, value: Any) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫–µ—à"""
        self.cache_data[key] = value
        self._save_cache()

    def is_changed(self, key: str, data: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–º–µ–Ω–∏–ª–∏—Å—å –ª–∏ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        current_hash = hashlib.md5(data.encode('utf-8')).hexdigest()
        cached_entry = self.get(key)

        if cached_entry and cached_entry.get('hash') == current_hash:
            print(f"üìã [{key}] –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à")
            return False

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à
        self.set(key, {
            'hash': current_hash,
            'last_updated': datetime.now().isoformat()
        })
        return True


# ============================================================================
# –û–ë–†–ê–ë–û–¢–ß–ò–ö–ò –î–ê–ù–ù–´–• / DATA PROCESSORS
# ============================================================================

class TextFieldProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π"""

    def __init__(self, text_fields: Optional[List[str]] = None, auto_detect: bool = True):
        self.text_fields = text_fields or []
        self.auto_detect = auto_detect

    def process(self, data: pd.DataFrame) -> pd.DataFrame:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è"""
        if data.empty:
            return data

        df = data.copy()
        text_columns = self._detect_text_columns(df) if self.auto_detect else self.text_fields

        if text_columns:
            print(f"üî§ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {', '.join(text_columns)}")

            for column in text_columns:
                if column in df.columns:
                    # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ \r, –æ—Å—Ç–∞–≤–ª—è–µ–º \n
                    df[column] = df[column].astype(str).str.replace('\r', '', regex=False)

        return df

    def _detect_text_columns(self, df: pd.DataFrame) -> List[str]:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏"""
        text_columns = []
        for column in df.columns:
            # ‚úÖ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            if pd.api.types.is_numeric_dtype(df[column]):
                continue

            sample_data = df[column].dropna().astype(str).head(10)
            has_newlines = any('\n' in str(value) for value in sample_data)
            avg_length = sample_data.str.len().mean() if len(sample_data) > 0 else 0

            if has_newlines or avg_length > 50:
                text_columns.append(column)

        return text_columns


class FilenameProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö –∏–º–µ–Ω —Ñ–∞–π–ª–æ–≤"""

    def __init__(self, transliterate: bool = True):
        self.transliterate = transliterate

    def create_safe_filename(self, sheet_name: str, page_num: int) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞"""
        if not self.transliterate:
            clean_name = re.sub(r'[^a-zA-Z0-9\-\u0400-\u04FF]', '_', sheet_name)
            clean_name = re.sub(r'_+', '_', clean_name).strip('_').lower()
            safe_name = clean_name if clean_name else 'sheet'
        else:
            if SLUGIFY_AVAILABLE:
                safe_name = slugify(
                    sheet_name,
                    max_length=50,
                    word_boundary=True,
                    separator='_',
                    lowercase=True
                )
                safe_name = safe_name if safe_name else 'sheet'
            else:
                safe_name = self._basic_transliterate(sheet_name)

        return f"{safe_name}_{page_num:02d}"

    def _basic_transliterate(self, text: str) -> str:
        """–ë–∞–∑–æ–≤–∞—è —Ä—É—Å—Å–∫–∞—è —Ç—Ä–∞–Ω—Å–ª–∏—Ç–µ—Ä–∞—Ü–∏—è"""
        translit_dict = {
            '–∞': 'a', '–±': 'b', '–≤': 'v', '–≥': 'g', '–¥': 'd', '–µ': 'e', '—ë': 'yo',
            '–∂': 'zh', '–∑': 'z', '–∏': 'i', '–π': 'y', '–∫': 'k', '–ª': 'l', '–º': 'm',
            '–Ω': 'n', '–æ': 'o', '–ø': 'p', '—Ä': 'r', '—Å': 's', '—Ç': 't', '—É': 'u',
            '—Ñ': 'f', '—Ö': 'h', '—Ü': 'ts', '—á': 'ch', '—à': 'sh', '—â': 'sch',
            '—ä': '', '—ã': 'y', '—å': '', '—ç': 'e', '—é': 'yu', '—è': 'ya'
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã
        for k, v in list(translit_dict.items()):
            if k != k.upper():
                translit_dict[k.upper()] = v.capitalize() if v else ''

        result = ''.join(translit_dict.get(char, char) for char in text)
        clean_name = re.sub(r'[^a-zA-Z0-9\-]', '_', result)
        clean_name = re.sub(r'_+', '_', clean_name).strip('_').lower()

        return clean_name if clean_name else 'sheet'


# ============================================================================
# –≠–ö–°–ü–û–†–¢–ï–†–´ –î–ê–ù–ù–´–• / DATA EXPORTERS
# ============================================================================

class BaseExporter(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä–æ–≤"""

    def __init__(self, output_dir: str, format_name: str):
        self.output_dir = os.path.join(output_dir, format_name)
        self.format_name = format_name
        os.makedirs(self.output_dir, exist_ok=True)

    @abstractmethod
    def get_extension(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        pass

    @abstractmethod
    def export_data(self, data: pd.DataFrame, filepath: str, **kwargs) -> None:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª"""
        pass

    def export(self, data: pd.DataFrame, filename: str, **kwargs) -> str:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É"""
        if not filename.endswith(f'.{self.get_extension()}'):
            filename = f"{filename}.{self.get_extension()}"

        filepath = os.path.join(self.output_dir, filename)
        self.export_data(data, filepath, **kwargs)
        return filepath


class CSVExporter(BaseExporter):
    """–≠–∫—Å–ø–æ—Ä—Ç–µ—Ä –≤ CSV"""

    def __init__(self, output_dir: str):
        super().__init__(output_dir, 'csv')

    def get_extension(self) -> str:
        return 'csv'

    def export_data(self, data: pd.DataFrame, filepath: str, **kwargs) -> None:
        separator = kwargs.get('separator', ';')
        encoding = kwargs.get('encoding', 'utf-8')
        data.to_csv(filepath, sep=separator, encoding=encoding, index=False, quoting=2)


class JSONExporter(BaseExporter):
    """–≠–∫—Å–ø–æ—Ä—Ç–µ—Ä –≤ JSON"""

    def __init__(self, output_dir: str):
        super().__init__(output_dir, 'json')

    def get_extension(self) -> str:
        return 'json'

    def export_data(self, data: pd.DataFrame, filepath: str, **kwargs) -> None:
        orient = kwargs.get('orient', 'records')
        indent = kwargs.get('indent', 4)

        data_dict = data.to_dict(orient)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data_dict, f, ensure_ascii=False, indent=indent)


class ExcelExporter(BaseExporter):
    """–≠–∫—Å–ø–æ—Ä—Ç–µ—Ä –≤ Excel"""

    def __init__(self, output_dir: str):
        super().__init__(output_dir, 'excel')

    def get_extension(self) -> str:
        return 'xlsx'

    def export_data(self, data: pd.DataFrame, filepath: str, **kwargs) -> None:
        if not XLSX_AVAILABLE:
            raise ImportError("openpyxl required for Excel export")

        sheet_name = kwargs.get('sheet_name', 'Sheet1')
        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
            data.to_excel(writer, sheet_name=sheet_name, index=False)


class YAMLExporter(BaseExporter):
    """–≠–∫—Å–ø–æ—Ä—Ç–µ—Ä –≤ YAML"""

    def __init__(self, output_dir: str):
        super().__init__(output_dir, 'yaml')

    def get_extension(self) -> str:
        return 'yml'

    def export_data(self, data: pd.DataFrame, filepath: str, **kwargs) -> None:
        data_dict = data.to_dict('records')
        with open(filepath, 'w', encoding='utf-8') as f:
            yaml.dump(data_dict, f, default_flow_style=False, allow_unicode=True, indent=2)


# ============================================================================
# –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –û–¢–ß–ï–¢–´ / STATISTICS & REPORTS
# ============================================================================

class StatsCollector:
    """–°–±–æ—Ä—â–∏–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""

    def __init__(self):
        self.stats = {
            'start_time': datetime.now(),
            'end_time': None,
            'total_sheets': 0,
            'processed_sheets': 0,
            'skipped_sheets': 0,
            'cached_sheets': 0,
            'batch_requests': 0,
            'formats_used': [],
            'files_created': [],
            'sheets_details': []
        }

    def add_file(self, format_name: str, filepath: str) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–∑–¥–∞–Ω–Ω–æ–º —Ñ–∞–π–ª–µ"""
        try:
            size = os.path.getsize(filepath)
        except OSError:
            size = 0

        self.stats['files_created'].append({
            'format': format_name,
            'filepath': filepath,
            'size_bytes': size
        })

    def add_sheet_detail(self, name: str, processed: bool, rows: int = 0,
                         columns: int = 0, from_cache: bool = False,
                         skip_reason: str = '') -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–µ—Ç–∞–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–∏—Å—Ç–∞"""
        self.stats['sheets_details'].append({
            'name': name,
            'processed': processed,
            'rows': rows,
            'columns': columns,
            'from_cache': from_cache,
            'skip_reason': skip_reason
        })

    def finalize(self) -> Dict[str, Any]:
        """–ó–∞–≤–µ—Ä—à–∞–µ—Ç —Å–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—á–µ—Ç"""
        self.stats['end_time'] = datetime.now()

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø–æ —Ñ–æ—Ä–º–∞—Ç–∞–º
        files_by_format = {}
        total_size = 0

        for file_info in self.stats['files_created']:
            format_name = file_info['format']
            if format_name not in files_by_format:
                files_by_format[format_name] = {'count': 0, 'total_size': 0}

            files_by_format[format_name]['count'] += 1
            files_by_format[format_name]['total_size'] += file_info['size_bytes']
            total_size += file_info['size_bytes']

        duration = self.stats['end_time'] - self.stats['start_time']

        return {
            'summary': {
                'duration': str(duration).split('.')[0],
                'total_sheets': self.stats['total_sheets'],
                'processed_sheets': self.stats['processed_sheets'],
                'cached_sheets': self.stats['cached_sheets'],
                'skipped_sheets': self.stats['skipped_sheets'],
                'batch_requests': self.stats['batch_requests'],
                'files_created': len(self.stats['files_created']),
                'total_size_bytes': total_size
            },
            'files_by_format': files_by_format,
            'sheets_details': self.stats['sheets_details']
        }


# ============================================================================
# –ì–õ–ê–í–ù–´–ô –ö–û–û–†–î–ò–ù–ê–¢–û–† / MAIN COORDINATOR
# ============================================================================

class ModularSheetsConverter:
    """–ì–ª–∞–≤–Ω—ã–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä - —Å–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ –º–æ–¥—É–ª–∏ –≤–º–µ—Å—Ç–µ"""

    def __init__(self,
                 config: Config,
                 cache: Optional[CacheProvider] = None,
                 exporters: Optional[Dict[str, BaseExporter]] = None,
                 processors: Optional[List[DataProcessor]] = None):

        self.config = config
        self.client = GoogleSheetsClient(config)
        self.cache = cache or FileCache(config.cache_file)
        self.exporters = exporters or {}
        self.processors = processors or []
        self.stats = StatsCollector()
        self.filename_processor = FilenameProcessor(transliterate=True)

        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if not self.exporters:
            self._register_default_exporters()

    def _register_default_exporters(self) -> None:
        """–†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        self.exporters = {
            'csv': CSVExporter(self.config.output_dir),
            'json': JSONExporter(self.config.output_dir),
            'yaml': YAMLExporter(self.config.output_dir)
        }

        if XLSX_AVAILABLE:
            self.exporters['xlsx'] = ExcelExporter(self.config.output_dir)

    def register_exporter(self, name: str, exporter: BaseExporter) -> None:
        """–†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä"""
        self.exporters[name] = exporter

    def add_processor(self, processor: DataProcessor) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö"""
        self.processors.append(processor)

    def convert_spreadsheet(self, url: str, formats: List[str]) -> Dict[str, Any]:
        """
        üöÄ –ì–õ–ê–í–ù–´–ô –ú–ï–¢–û–î: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤—Å—é —Ç–∞–±–ª–∏—Ü—É —Å batch –∑–∞–≥—Ä—É–∑–∫–æ–π
        """
        print("üöÄ –ó–∞–ø—É—Å–∫ –º–æ–¥—É–ª—å–Ω–æ–≥–æ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞ Google Sheets")
        print("=" * 60)

        try:
            # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º ID –∏ –ø–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            sheet_id = self.client.extract_sheet_id(url)
            print(f"üîç ID —Ç–∞–±–ª–∏—Ü—ã: {sheet_id}")

            sheets_info = self.client.get_sheets_metadata(sheet_id)
            if not sheets_info:
                print("‚ùå –õ–∏—Å—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return self.stats.finalize()

            self.stats.stats['total_sheets'] = len(sheets_info)
            self.stats.stats['formats_used'] = formats

            # 2. Batch –∑–∞–≥—Ä—É–∑–∫–∞ –í–°–ï–• –ª–∏—Å—Ç–æ–≤ –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—Ä–æ—Å  
            print(f"\nüöÄ Batch –∑–∞–≥—Ä—É–∑–∫–∞ {len(sheets_info)} –ª–∏—Å—Ç–æ–≤...")
            sheets_data = self.client.batch_get_sheets_data(sheet_id, sheets_info)
            self.stats.stats['batch_requests'] = 1

            # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —ç–∫—Å–ø–æ—Ä—Ç –∫–∞–∂–¥–æ–≥–æ –ª–∏—Å—Ç–∞
            print(f"\nüîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(sheets_data)} –ª–∏—Å—Ç–æ–≤...")

            for page_num, sheet_info in enumerate(sheets_info, 1):
                sheet_name = sheet_info['name']

                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                sheet_result = sheets_data.get(sheet_name, (pd.DataFrame(), ""))
                df, raw_data = sheet_result

                if df.empty:
                    print(f"‚è≠Ô∏è [{sheet_name}] –ü—Ä–æ–ø—É—Å–∫ - –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                    self.stats.add_sheet_detail(sheet_name, False, skip_reason="–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                    self.stats.stats['skipped_sheets'] += 1
                    continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
                cache_key = f"{sheet_id}:{sheet_name}"
                if not self.cache.is_changed(cache_key, raw_data):
                    print(f"üìã [{sheet_name}] –ö–µ—à –∞–∫—Ç—É–∞–ª–µ–Ω")
                    self.stats.add_sheet_detail(sheet_name, True, len(df), len(df.columns), from_cache=True)
                    self.stats.stats['cached_sheets'] += 1
                    continue

                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                processed_df = self._process_data(df)

                # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤–æ –≤—Å–µ —Ñ–æ—Ä–º–∞—Ç—ã
                filename_base = self.filename_processor.create_safe_filename(sheet_name, page_num)
                exported_files = self._export_data(processed_df, filename_base, formats)

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self.stats.add_sheet_detail(sheet_name, True, len(df), len(df.columns))
                self.stats.stats['processed_sheets'] += 1

                print(f"‚úÖ [{sheet_name}] –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ {len(exported_files)} —Ñ–æ—Ä–º–∞—Ç–æ–≤")

            # 4. –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è
            final_stats = self.stats.finalize()
            self._print_summary(final_stats)

            return final_stats

        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return self.stats.finalize()

    def _process_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –≤—Å–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏"""
        processed_df = df
        for processor in self.processors:
            processed_df = processor.process(processed_df)
        return processed_df

    def _export_data(self, df: pd.DataFrame, filename_base: str, formats: List[str]) -> Dict[str, str]:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤–æ –≤—Å–µ —É–∫–∞–∑–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã"""
        exported_files = {}

        for format_name in formats:
            if format_name not in self.exporters:
                print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: {format_name}")
                continue

            try:
                exporter = self.exporters[format_name]
                filepath = exporter.export(df, filename_base)
                exported_files[format_name] = filepath
                self.stats.add_file(format_name, filepath)

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ {format_name}: {e}")

        return exported_files

    def _print_summary(self, stats: Dict[str, Any]) -> None:
        """–í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É"""
        summary = stats['summary']

        print("\n" + "=" * 60)
        print("üìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê")
        print("=" * 60)
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {summary['duration']}")
        print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ª–∏—Å—Ç–æ–≤: {summary['processed_sheets']}/{summary['total_sheets']}")
        print(f"üìÇ –ò–∑ –∫–µ—à–∞: {summary['cached_sheets']}")
        print(f"üöÄ Batch –∑–∞–ø—Ä–æ—Å–æ–≤: {summary['batch_requests']}")
        print(f"üìÅ –°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {summary['files_created']}")

        # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–æ–≤
        total_mb = summary['total_size_bytes'] / (1024 * 1024)
        if total_mb > 1:
            print(f"üíæ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {total_mb:.1f} MB")
        else:
            print(f"üíæ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {summary['total_size_bytes'] / 1024:.1f} KB")


# ============================================================================
# CONVENIENCE FUNCTIONS / –§–£–ù–ö–¶–ò–ò –£–î–û–ë–°–¢–í–ê
# ============================================================================

def quick_convert(url: str,
                  formats: List[str] = None,
                  output_dir: str = "output",
                  api_key: Optional[str] = None,
                  enable_cache: bool = True) -> Dict[str, Any]:
    """–ë—ã—Å—Ç—Ä–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""

    if formats is None:
        formats = ['csv', 'json']

    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = Config(api_key=api_key, output_dir=output_dir)

    # –°–æ–∑–¥–∞–µ–º –∫–µ—à –µ—Å–ª–∏ –Ω—É–∂–µ–Ω
    cache = FileCache(config.cache_file) if enable_cache else None

    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä
    converter = ModularSheetsConverter(config=config, cache=cache)

    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    converter.add_processor(TextFieldProcessor(auto_detect=True))

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
    return converter.convert_spreadsheet(url, formats)


def convert_with_validation(url: str,
                            expected_headers: List[str],
                            text_fields: List[str] = None,
                            formats: List[str] = None,
                            output_dir: str = "output") -> Dict[str, Any]:
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""

    if formats is None:
        formats = ['csv', 'json']

    config = Config(output_dir=output_dir)
    converter = ModularSheetsConverter(config=config)

    # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–ª–∏–¥–∞—Ç–æ—Ä –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–º–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–∞–∫ DataProcessor)
    if text_fields:
        converter.add_processor(TextFieldProcessor(text_fields=text_fields, auto_detect=False))

    return converter.convert_spreadsheet(url, formats)


# ============================================================================
# –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø / EXAMPLE USAGE
# ============================================================================

def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥—É–ª—å–Ω–æ–≥–æ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞"""

    if len(sys.argv) > 1:
        # CLI —Ä–µ–∂–∏–º
        url = sys.argv[1]
        formats = ['csv', 'json', 'xlsx', 'yaml']

        print("üöÄ –ó–∞–ø—É—Å–∫ –≤ CLI —Ä–µ–∂–∏–º–µ")
        result = quick_convert(url, formats, enable_cache=True)

        if result['summary']['processed_sheets'] > 0:
            print("üéâ –£—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        else:
            print("üí• –û—à–∏–±–∫–∏ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ")
            sys.exit(1)

    else:
        # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
        url = input("üìù –í–≤–µ–¥–∏—Ç–µ URL Google –¢–∞–±–ª–∏—Ü—ã: ").strip()

        if not url:
            url = "https://docs.google.com/spreadsheets/d/1RagVK40gWitjfQE-_wBD8HnSaeDGGMZJ2uWfICLRqFQ/edit"
            print(f"üß™ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É: {url}")

        # –í—ã–±–æ—Ä —Ñ–æ—Ä–º–∞—Ç–æ–≤
        print("\n–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: csv, json, xlsx, yaml")
        formats_input = input("–í—ã–±–µ—Ä–∏—Ç–µ —Ñ–æ—Ä–º–∞—Ç—ã (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é): ").strip()

        if formats_input:
            formats = [f.strip() for f in formats_input.split(',')]
        else:
            formats = ['csv', 'json']

        print(f"‚úÖ –í—ã–±—Ä–∞–Ω—ã —Ñ–æ—Ä–º–∞—Ç—ã: {formats}")

        # –ó–∞–ø—É—Å–∫ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
        result = quick_convert(url, formats, enable_cache=True)

        print("\nüéâ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: output/")


if __name__ == "__main__":
    main()
