#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–∞ production —Å–µ—Ä–≤–µ—Ä–µ
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç: python import_knowledge_base.py --backup-dir exports/knowledge_base_backup_20250123
"""

import argparse
import json
import logging
import os
import sys
import zipfile
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.admin.knowledge_base_manager import knowledge_base_manager
from src.ai.knowledge_base import knowledge_base

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def main():
    parser = argparse.ArgumentParser(description="–ò–º–ø–æ—Ä—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–∞ production —Å–µ—Ä–≤–µ—Ä")
    parser.add_argument("--backup-dir", required=True, help="–ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –±—ç–∫–∞–ø–æ–º –∏–ª–∏ ZIP —Ñ–∞–π–ª—É")
    parser.add_argument(
        "--force", action="store_true", help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–º–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"
    )
    parser.add_argument(
        "--verify-only", action="store_true", help="–¢–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∫–∞ –±—ç–∫–∞–ø–∞ –±–µ–∑ –∏–º–ø–æ—Ä—Ç–∞"
    )

    args = parser.parse_args()

    try:
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–º–ø–æ—Ä—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –Ω–∞ production —Å–µ—Ä–≤–µ—Ä")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Ç—å –∫ –±—ç–∫–∞–ø—É
        backup_path = Path(args.backup_dir)
        if not backup_path.exists():
            logger.error(f"‚ùå –ü—É—Ç—å –∫ –±—ç–∫–∞–ø—É –Ω–µ –Ω–∞–π–¥–µ–Ω: {backup_path}")
            return 1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –±—ç–∫–∞–ø–∞
        if backup_path.is_file() and backup_path.suffix == ".zip":
            logger.info(f"üì¶ –ù–∞–π–¥–µ–Ω ZIP –±—ç–∫–∞–ø: {backup_path}")
            backup_info = verify_zip_backup(backup_path)
        elif backup_path.is_dir():
            logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ –±—ç–∫–∞–ø–∞: {backup_path}")
            backup_info = verify_directory_backup(backup_path)
        else:
            logger.error("‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –±—ç–∫–∞–ø–∞")
            return 1

        if not backup_info["valid"]:
            logger.error(f"‚ùå –ë—ç–∫–∞–ø –Ω–µ–≤–∞–ª–∏–¥–µ–Ω: {backup_info['error']}")
            return 1

        logger.info(f"‚úÖ –ë—ç–∫–∞–ø –≤–∞–ª–∏–¥–µ–Ω:")
        logger.info(f"   ‚Ä¢ –ó–∞–ø–∏—Å–µ–π: {backup_info['total_items']}")
        logger.info(f"   ‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {list(backup_info['categories'].keys())}")
        logger.info(f"   ‚Ä¢ –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {list(backup_info['sources'].keys())}")

        if args.verify_only:
            logger.info("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
            return 0

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        existing_overview = knowledge_base_manager.get_knowledge_base_overview()
        if existing_overview["success"] and existing_overview["total_documents"] > 0:
            if not args.force:
                logger.warning("‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π:")
                logger.warning(f"   ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {existing_overview['total_documents']}")
                logger.warning("   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --force –¥–ª—è –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–º–µ–Ω—ã")

                confirm = input("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∏ –∑–∞–º–µ–Ω–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É? (yes/no): ")
                if confirm.lower() not in ["yes", "y", "–¥–∞"]:
                    logger.info("–ò–º–ø–æ—Ä—Ç –æ—Ç–º–µ–Ω–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                    return 0

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∏–º–ø–æ—Ä—Ç
        logger.info("üì• –ù–∞—á–∏–Ω–∞–µ–º –∏–º–ø–æ—Ä—Ç...")

        if backup_path.suffix == ".zip":
            # –ò–∑–≤–ª–µ–∫–∞–µ–º ZIP –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
            import tempfile

            with tempfile.TemporaryDirectory() as temp_dir:
                logger.info("üì¶ –ò–∑–≤–ª–µ–∫–∞–µ–º ZIP –∞—Ä—Ö–∏–≤...")
                with zipfile.ZipFile(backup_path, "r") as zip_ref:
                    zip_ref.extractall(temp_dir)

                # –ò—â–µ–º –ø–∞–ø–∫—É —Å –±—ç–∫–∞–ø–æ–º
                extracted_dirs = [d for d in Path(temp_dir).iterdir() if d.is_dir()]
                if extracted_dirs:
                    import_path = str(extracted_dirs[0])
                else:
                    import_path = temp_dir

                result = knowledge_base_manager.import_from_backup(import_path)
        else:
            result = knowledge_base_manager.import_from_backup(str(backup_path))

        if result["success"]:
            logger.info("‚úÖ –ò–º–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            logger.info(f"   ‚Ä¢ –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {result['imported_count']} –∑–∞–ø–∏—Å–µ–π")
            logger.info(f"   ‚Ä¢ –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result['total_items']} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            new_overview = knowledge_base_manager.get_knowledge_base_overview()
            if new_overview["success"]:
                logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–∞:")
                logger.info(f"   ‚Ä¢ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {new_overview['total_documents']}")
                for category, count in new_overview["categories"].items():
                    logger.info(f"   ‚Ä¢ {category}: {count}")

            logger.info("\nüéâ –ò–ú–ü–û–†–¢ –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
            logger.info("\nüìã –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
            logger.info("   ‚Ä¢ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ AI —Ä–µ–∂–∏–º –≤ –±–æ—Ç–µ")
            logger.info("   ‚Ä¢ –í—ã–ø–æ–ª–Ω–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É /stats –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
            logger.info("   ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–º–∞–Ω–¥—É /analytics")

            return 0
        else:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {result['error']}")
            return 1

    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
        return 1


def verify_zip_backup(zip_path: Path) -> dict:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å ZIP –±—ç–∫–∞–ø–∞"""
    try:
        with zipfile.ZipFile(zip_path, "r") as zip_ref:
            files = zip_ref.namelist()

            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã
            has_json = any("knowledge_base.json" in f for f in files)
            has_metadata = any("backup_info.json" in f for f in files)
            has_instructions = any("DEPLOYMENT_INSTRUCTIONS.md" in f for f in files)

            if not (has_json and has_metadata):
                return {
                    "valid": False,
                    "error": "ZIP –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤ (knowledge_base.json, backup_info.json)",
                }

            # –ß–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata_file = None
            for f in files:
                if "backup_info.json" in f:
                    metadata_file = f
                    break

            if metadata_file:
                with zip_ref.open(metadata_file) as f:
                    metadata = json.loads(f.read().decode("utf-8"))

                    return {
                        "valid": True,
                        "total_items": metadata.get("total_items", 0),
                        "categories": metadata.get("categories", {}),
                        "sources": metadata.get("sources", {}),
                        "backup_version": metadata.get("backup_version", "unknown"),
                    }

            return {"valid": False, "error": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"}

    except Exception as e:
        return {"valid": False, "error": f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è ZIP: {e}"}


def verify_directory_backup(dir_path: Path) -> dict:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ø–∞–ø–∫–∏ —Å –±—ç–∫–∞–ø–æ–º"""
    try:
        json_file = dir_path / "knowledge_base.json"
        metadata_file = dir_path / "backup_info.json"

        if not json_file.exists():
            return {"valid": False, "error": "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç knowledge_base.json"}

        if not metadata_file.exists():
            return {"valid": False, "error": "–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç backup_info.json"}

        # –ß–∏—Ç–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        with open(metadata_file, "r", encoding="utf-8") as f:
            metadata = json.load(f)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º JSON –¥–∞–Ω–Ω—ã–µ
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)

            if not isinstance(data, list):
                return {
                    "valid": False,
                    "error": "knowledge_base.json –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–∞—Å—Å–∏–≤ –¥–∞–Ω–Ω—ã—Ö",
                }

        return {
            "valid": True,
            "total_items": metadata.get("total_items", len(data)),
            "categories": metadata.get("categories", {}),
            "sources": metadata.get("sources", {}),
            "backup_version": metadata.get("backup_version", "unknown"),
        }

    except Exception as e:
        return {"valid": False, "error": f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –ø–∞–ø–∫–∏: {e}"}


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
